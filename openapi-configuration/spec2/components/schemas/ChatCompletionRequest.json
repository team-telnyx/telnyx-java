{
  "properties": {
    "messages": {
      "items": {
        "$ref": "./ChatCompletionSystemMessageParam.json"
      },
      "type": "array",
      "example": [
        {
          "role": "system",
          "content": "You are a friendly chatbot."
        },
        {
          "role": "user",
          "content": "Hello, world!"
        }
      ],
      "description": "A list of the previous chat messages for context."
    },
    "model": {
      "type": "string",
      "default": "meta-llama/Meta-Llama-3.1-8B-Instruct",
      "description": "The language model to chat with. If you are optimizing for speed + price, try `meta-llama/Meta-Llama-3.1-8B-Instruct`. For quality, try `meta-llama/Meta-Llama-3.1-70B-Instruct`. Or explore our [LLM Library](https://telnyx.com/products/llm-library)."
    },
    "stream": {
      "type": "boolean",
      "default": false,
      "description": "Whether or not to stream data-only server-sent events as they become available."
    },
    "temperature": {
      "type": "number",
      "default": 0.1,
      "description": "Adjusts the \"creativity\" of the model. Lower values make the model more deterministic and repetitive, while higher values make the model more random and creative."
    },
    "max_tokens": {
      "type": "integer",
      "description": "Maximum number of completion tokens the model should generate."
    },
    "tools": {
      "items": {
        "oneOf": [
          {
            "$ref": "./ChatCompletionToolParam.json"
          },
          {
            "$ref": "./Retrieval.json"
          }
        ]
      },
      "type": "array",
      "description": "The `function` tool type follows the same schema as the [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat). The `retrieval` tool type is unique to Telnyx. You may pass a list of [embedded storage buckets](https://developers.telnyx.com/api/inference/inference-embedding/post-embedding) for retrieval-augmented generation."
    },
    "tool_choice": {
      "type": "string",
      "enum": [
        "none",
        "auto",
        "required"
      ]
    },
    "response_format": {
      "description": "Use this is you want to guarantee a JSON output without defining a schema. For control over the schema, use `guided_json`.",
      "$ref": "./ChatCompletionResponseFormatParam.json"
    },
    "guided_json": {
      "type": "object",
      "description": "Must be a valid JSON schema. If specified, the output will follow the JSON schema."
    },
    "guided_regex": {
      "type": "string",
      "description": "If specified, the output will follow the regex pattern."
    },
    "guided_choice": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "If specified, the output will be exactly one of the choices."
    },
    "min_p": {
      "type": "number",
      "description": "This is an alternative to `top_p` that [many prefer](https://github.com/huggingface/transformers/issues/27670). Must be in [0, 1]."
    },
    "n": {
      "type": "number",
      "description": "This will return multiple choices for you instead of a single chat completion."
    },
    "use_beam_search": {
      "type": "boolean",
      "default": false,
      "description": "Setting this to `true` will allow the model to [explore more completion options](https://huggingface.co/blog/how-to-generate#beam-search). This is not supported by OpenAI."
    },
    "best_of": {
      "type": "integer",
      "description": "This is used with `use_beam_search` to determine how many candidate beams to explore."
    },
    "length_penalty": {
      "type": "number",
      "default": 1,
      "description": "This is used with `use_beam_search` to prefer shorter or longer completions."
    },
    "early_stopping": {
      "type": "boolean",
      "default": false,
      "description": "This is used with `use_beam_search`. If `true`, generation stops as soon as there are `best_of` complete candidates; if `false`, a heuristic is applied and the generation stops when is it very unlikely to find better candidates."
    },
    "logprobs": {
      "type": "boolean",
      "default": false,
      "description": "Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`."
    },
    "top_logprobs": {
      "type": "integer",
      "description": "This is used with `logprobs`. An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability."
    },
    "frequency_penalty": {
      "type": "number",
      "default": 0,
      "description": "Higher values will penalize the model from repeating the same output tokens."
    },
    "presence_penalty": {
      "type": "number",
      "default": 0,
      "description": "Higher values will penalize the model from repeating the same output tokens."
    },
    "top_p": {
      "type": "number",
      "description": "An alternative or complement to `temperature`. This adjusts how many of the top possibilities to consider."
    },
    "openai_api_key": {
      "type": "string",
      "description": "If you are using OpenAI models using our API, this is how you pass along your OpenAI API key."
    }
  },
  "type": "object",
  "required": [
    "messages"
  ],
  "title": "ChatCompletionRequest"
}